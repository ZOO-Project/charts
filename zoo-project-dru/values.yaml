# Default values for zoo-project-dru.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""


useKubeProxy: true
clusterAdminRoleName: cluster-admin

podAnnotations: {}

podSecurityContext: {}
# fsGroup: 2000

securityContext: {}
# capabilities:
#   drop:
#   - ALL
# readOnlyRootFilesystem: true
# runAsNonRoot: true
# runAsUser: 1000

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  # `hosturl` provides the opportunuity to specify the service URL when ingress is provided
  # external to this helm chart. If ingress is enabled then the service URL can be taken
  # from `ingress.hosts.host[0]`.
  # Thus, the value of the service URL is deduced in the following order of priority...
  #   * `ingress.hosturl`
  #   * `ingress.hosts.host[0]` if `ingress.enabled: true`
  #   * `http://localhost:8080` if all else fails
  # hosturl: https://myzoo.example.com
  className: ""
  annotations: {}
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources, such as Minikube. If you do want to specify resources, uncomment the following
# lines, adjust them as necessary, and remove the curly braces after 'resources:'.
# limits:
#   cpu: 100m
#   memory: 128Mi
# requests:
#   cpu: 100m
#   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# KEDA Event-driven autoscaling configuration
keda:
  # Enable KEDA as a subchart dependency
  enabled: false
  # KEDA subchart configuration
  # When enabled, KEDA will be deployed as part of this chart
  # See: https://github.com/kedacore/charts/tree/main/keda
  operator:
    #name: keda-operator
    replicaCount: 1
  metricsServer:
    replicaCount: 1
  webhooks:
    enabled: false
  
  # Eviction controller configuration (alternative to sidecar approach)
  evictionController:
    enabled: false  # Enable centralized eviction controller instead of individual sidecars
    image:
      repository: ghcr.io/zoo-project/zoofpm-eviction-controller
      tag: "latest"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi

  # Kyverno Policy Engine for enhanced protection
  kyverno:
    enabled: false  # Disabled as Kyverno is installed separately by Skaffold
    namespaceOverride: "kyverno-system"  # Dedicated namespace for Kyverno
    policies:
      zoofpmProtection:
        enabled: true  # Policy to protect zoofpm pods
        failurePolicy: "Enforce"  # Strictly block deletions (Enforce/Audit)
        background: false  # Real-time validation only
        protectZoofpm: true  # Prevent deletion of zoofpm pods
        allowedOperations: ["CREATE", "UPDATE"]  # Only these operations are allowed

  # ZOO-Project specific KEDA configuration
  scaleTargetRef:
    deployment: "zoofpm"  # Will be prefixed with release name
  pollingInterval: 10     # Check triggers every 10 seconds
  cooldownPeriod: 60      # Wait 1 minute before scaling down
  minReplicas: 0          # Allow scale-to-zero
  maxReplicas: 20         # Scale up to maximum 20 replicas
  idleReplicaCount: 0     # Number of replicas when idle (scale-to-zero)
  triggers:
    postgresql:
      enabled: true
      # STRICT query - Absolute protection of pods by IP
      # GUARANTEE: No pod with its IP in workers.host can be deleted
      query: |
        WITH active_worker_ips AS (
          -- IPs of pods that have active workers - ABSOLUTE PROTECTION
          SELECT DISTINCT host as protected_ip
          FROM workers 
          WHERE status = 1 AND host IS NOT NULL
        ),
        protected_count AS (
          -- MINIMUM number of pods to keep (those with workers)
          SELECT COUNT(*) as must_keep_minimum
          FROM active_worker_ips
        ),
        workload_needs AS (
          -- Calculation based on total workload
          SELECT 
            SUM(CASE WHEN status = 1 THEN 1 ELSE 0 END) as active_workers,
            -- Pods needed for workload (async_worker=10 per pod)
            -- Scale-to-zero if no active workers
            CASE 
              WHEN SUM(CASE WHEN status = 1 THEN 1 ELSE 0 END) > 0
              THEN GREATEST(CEIL(SUM(CASE WHEN status = 1 THEN 1 ELSE 0 END)::decimal / 10), 1)
              ELSE 0  -- Scale-to-zero if no workers
            END as pods_for_workload
          FROM workers
          WHERE host IS NOT NULL
        ),
        service_needs AS (
          -- Pods needed for running services
          SELECT 
            COUNT(*) as running_services,
            -- Only if workers exist AND services
            CASE 
              WHEN EXISTS(SELECT 1 FROM workers WHERE status = 1 AND host IS NOT NULL) 
                AND COUNT(*) > 0 
              THEN GREATEST(CEIL(COUNT(*)::decimal / 10), 1)
              ELSE 0  -- Scale-to-zero if no active workers
            END as pods_for_services
          FROM services 
          WHERE end_time IS NULL AND fstate NOT IN ('Succeeded', 'Failed')
        )
        SELECT 
          -- Hybrid logic: active workers AND pending services
          GREATEST(
            -- Calculation based on active workers (1 pod per 10 workers)
            CASE 
              WHEN (SELECT COUNT(*) FROM workers WHERE status = 1 AND host IS NOT NULL) > 0
              THEN CEIL((SELECT COUNT(*)::decimal FROM workers WHERE status = 1 AND host IS NOT NULL) / 10)
              ELSE 0
            END,
            -- Calculation based on active services (1 pod per 5 services)
            CASE 
              WHEN (SELECT COUNT(*) FROM services WHERE end_time IS NULL AND fstate NOT IN ('Succeeded', 'Failed')) > 0
              THEN CEIL((SELECT COUNT(*)::decimal FROM services WHERE end_time IS NULL AND fstate NOT IN ('Succeeded', 'Failed')) / 5)
              ELSE 0
            END,
            -- Scale-to-zero if no activity
            0
          )
      targetQueryValue: "1.0"  # 1:1 scaling - no over-allocation
      activationTargetQueryValue: "0.1"  # Scale-up at the slightest activity
      host: "" # Will be auto-generated from postgresql service
      port: "5432"
      dbName: "" # Will be taken from global.postgresql.auth.database
      userName: "" # Will be taken from global.postgresql.auth.username
      sslmode: "disable"
    rabbitmq:
      enabled: true
      protocol: "amqp"
      queueName: "zoo_service_queue"
      mode: "QueueLength"
      value: "1"
      host: "" # Will be auto-generated from rabbitmq service

nodeSelector: {}

tolerations: []

affinity: {}

zoo:
  promoteHead: true
  detectEntrypoint: false
  rabbitmq:
    definitions: "files/rabbitmq/definitions.json.tpl"

zoofpm:
  image:
    repository: zooproject/zoo-project
    pullPolicy: IfNotPresent
    tag: dru-d4b07ce3d7e12a0d3a56d1a3627052a9babe85d7
  autoscaling:
    enabled: false
  replicaCount: 1
  extraMountPoints: []

zookernel:
  env: {}
    # ZOO_OUTPUT_FORMAT: stac-collection
  image:
    repository: zooproject/zoo-project
    pullPolicy: IfNotPresent
    tag: dru-d4b07ce3d7e12a0d3a56d1a3627052a9babe85d7
  extraMountPoints: []

notifications:
  enabled: false
  ksink:
    kind: Broker
    namespace: default
    name: default

workflow:
  storageClass: standard
  defaultVolumeSize: "10190"
  defaultMaxRam: "1024"
  defaultMaxCores: "2"
  calrissianImage: "terradue/calrissian:0.12.0"
  imagePullSecrets: {}
  # auths:
  #  fake\.registry\.io:
  #    username: fakeuser
  #    password: fakepassword
  #    email: fakeuser@fake.registry.io
  #    auth: ""
  additionalImagePullSecrets: []
  nodeSelector: {}
  env: {}
  inputs: {}
  additionalInputs: {}
  dedicatedNamespace:
    enabled: false
    #name: "my-dedicated-namespace"
    #serviceAccount: "my-dedicated-service-account"
  argo:
    enabled: false
    storageClass: standard
    defaultVolumeSize: "12Gi"
    defaultMaxRam: "2Gi"
    wfServer: "http://argo-server.zoo.svc.cluster.local:2746"
    wfToken: "" # Token will be retrieved automatically during deployment
    wfSynchronizationCm: "semaphore-argo-cwl-runner-stage-in-out"
    CwlRunnerTemplare: "argo-cwl-runner-stage-in-out"
    CwlRunnerEndpoint: "calrissian-runner"

postgresql:
  # Configuration for official PostgreSQL Docker image
  enabled: true
  name: postgresql-db
  serviceName: postgresql-db-service

  # Security configuration
  useSecret: false  # Set to true to store credentials in a Kubernetes Secret
  secretName: ""    # Optional: specify an existing secret name, otherwise auto-generated

  image:
    repository: postgres
    tag: "16-alpine"
    pullPolicy: IfNotPresent

  # Resources configuration
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 250m
      memory: 256Mi

  # Persistence configuration
  persistence:
    enabled: true
    volumeName: postgresql-data
    mountPath: /var/lib/postgresql/data
    accessMode: ReadWriteOnce
    size: 8Gi
    storageClass: ""

  # Init scripts configuration (keep existing ConfigMap)
  primary:
    initdb:
      scriptsConfigMap: "postgresql-primary-init-scripts"

global:
  postgresql:
    auth:
      username: zoo
      password: zoo
      database: zoo

    service:
      ports:
        postgresql: "5432"

# rabbitmq
rabbitmq:
  enabled: true
  image:
    repository: rabbitmq
    tag: "4.1.4-alpine"
    pullPolicy: IfNotPresent

  replicas: 1

  # Auto-setup: automatically enable management plugin and import definitions
  autoSetup:
    enabled: true

  auth:
    username: "zoo"
    password: "CHANGEME"
    vhost: "/"

  clustering:
    enabled: false

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi

  persistence:
    enabled: true
    size: 8Gi
    storageClass: ""

  # RabbitMQ configuration
  config: |
    ## Management plugin
    management.tcp.port = 15672

    ## Disable disk free limit (for development)
    disk_free_limit.absolute = 1GB

    ## Enable logging
    log.console = true
    log.console.level = info

    ## Default user configuration
    default_user = zoo
    default_pass = CHANGEME
    default_vhost = /
    default_user_tags.administrator = true
    default_permissions.configure = .*
    default_permissions.read = .*
    default_permissions.write = .*

persistence:
  enabled: true
  # existingUserDataClaim:
  # existingProcServicesClaim:
  ## @param persistence.storageClass Storage class of backing PVC
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: ""
  userDataAccessMode: ReadWriteOnce
  userDataSize: 10Gi

  ## @param persistence.procServicesAccessMode Access mode of the PVC
  procServicesAccessMode: ReadWriteMany
  ## @param persistence.procServicesStorageClass Storage class of backing PVC
  ## require ReadWriteMany access mode
  procServicesStorageClass: standard
  procServicesSize: 5Gi
  ## @param persistence.tmpPath Path to mount the PVC
  servicesNamespacePath: /opt/zooservices_user

  ## @param persistence.tmpAccessMode Access mode of the PVC
  tmpAccessMode: ReadWriteMany
  ## @param persistence.tmpStorageClass Storage class of backing PVC
  ## require ReadWriteMany access mode
  tmpStorageClass: standard
  ## @param persistence.tmpSize Size of the PVC
  tmpSize: 2Gi
  ## @param persistence.tmpPath Path to mount the PVC
  tmpPath: /tmp/zTmp

cookiecutter:
  templateUrl: https://github.com/EOEPCA/eoepca-proc-service-template.git
  templateBranch: feature/python3.8

minio:
  enabled: false
  # Configuration for official MinIO chart (https://charts.min.io/)
  mode: standalone  # or 'distributed' for production
  replicas: 1

  # Authentication
  rootUser: minio-admin
  rootPassword: minio-secret-password

  # Backward compatibility: keep s3-service name
  fullnameOverride: "s3-service"

  # Create default buckets after install
  buckets:
    - name: eoepca
      policy: none
      purge: false
    - name: results
      policy: none
      purge: false

  # Resources
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

  # Persistence
  persistence:
    enabled: true
    size: 10Gi
    storageClass: ""  # Use default storage class

  # Service configuration
  service:
    type: ClusterIP
    port: 9000

  # Console (web UI)
  consoleService:
    type: ClusterIP
    port: 9001

  # Environment variables
  environment:
    MINIO_BROWSER: "on"
    MINIO_DOMAIN: ""

  # Security context
  securityContext:
    enabled: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

websocketd:
  enabled: false
  port: 8888
  image:
    repository: zooproject/websocketd
    pullPolicy: IfNotPresent
    tag: 67449315857b54bbc970f02c7aa4fd10a94721f0

redis: 
  # Configuration for official Redis Docker image
  enabled: false
  name: redis-db
  serviceName: redis-db-service
  port: 6379

  image:
    repository: redis
    tag: "alpine3.22"
    pullPolicy: IfNotPresent

  # Authentication configuration
  auth:
    enabled: false
    password: ""

  # Resources configuration
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Persistence configuration
  persistence:
    enabled: true
    volumeName: redis-data
    accessMode: ReadWriteOnce
    size: 4Gi
    storageClass: ""

iam: 
  enabled: true
  openIdConnectUrl: https://auth.geolabs.fr/realms/zooproject/.well-known/openid-configuration
  type: openIdConnect
  name: OpenIDAuth
  realm: ZOO-Project-secured-access
  openeoAuth:
    enabled: false
    title: OpenId Connect Secured Access
    grant_types:
      - implicit
      - authorization_code+pkce
      - urn:ietf:params:oauth:grant-type:device_code+pkce
    redirect_uris:
      - https://m-mohr.github.io/gdc-web-editor/
      - https://editor.openeo.org/

webui:
  enabled: false
  url: http://localhost:3058
  port: 3000
  image:
    repository: zooproject/nuxt-client
    pullPolicy: IfNotPresent
    tag: 0.0.2
  enforce: false
  oidc:
    issuer: https://auth.geolabs.fr/realms/zooproject
    remoteUserClaim: email
    providerTokenEndpointAuth: client_secret_basic
    authVerifyJwksUri: https://auth.geolabs.fr/realms/zooproject/protocol/openid-connect/certs
    scope: "openid email"

argo:
  enabled: false
  # Control whether to install CRDs or assume they are already installed
  installCRDs: true
  # Options for automatic token management
  autoTokenManagement: true  # Enable automatic token retrieval
  restartOnTokenUpdate: false  # Restart pods after token update
  # Service account configuration for Argo workflows
  serviceAccount:
    name: "argo-workflow"
  # Ingress configuration for Argo Workflows UI
  ingress:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # cert-manager.io/cluster-issuer: "letsencrypt-prod"
    hosts:
      - host: argo-workflows.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
      # - secretName: argo-workflows-tls
      #   hosts:
      #     - argo-workflows.local
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
  # Argo Events configuration for workflow monitoring
  events:
    enabled: false
    eventBus:
      enabled: false
      name: "default"
      type: "jetstream"  # or "nats"
      jetstream:
        version: "latest"
        replicas: 1
      nats:
        replicas: 1
        auth: "token"

# Argo Workflows configuration
argo-workflows:
  enabled: false
  server:
    enabled: false
  global:
    namespace: zoo
  cwlRunner:
    enabled: true
    defaultResources:
      maxRam: "8G"
      maxCores: "8"
  minio:
    enabled: false
    external:
      enabled: true
      endpoint: s3-service.zoo.svc.cluster.local:9000
      secure: false
      accessKeySecret:
        name: s3-service
        key: rootUser
      secretKeySecret:
        name: s3-service
        key: rootPassword
  stageInOut:
    enabled: true

# Monitoring configuration
monitoring:
  enabled: false
  # Disable problematic Kubernetes component targets for local development
  disableProblematicTargets: false
  # Disable node-exporter patch when port is already configured in values
  nodeExporterPatchEnabled: false
  kube-prometheus-stack:
    prometheusOperator:
      podSecurityPolicy:
        enabled: false
    crds:
      enabled: true
    prometheus:
      prometheusSpec:
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        ruleSelector: {}
    grafana:
      defaultDashboardsEnabled: true
      adminPassword: admin
    prometheus-node-exporter:
      enabled: false
      # Configuration for Docker Desktop when enabled
      hostRootFsMount:
        enabled: true
        mountPropagation: ""  # Empty = None, for Docker Desktop
      hostProcFsMount:
        mountPropagation: ""
      hostSysFsMount:
        mountPropagation: ""
    # Disable PodSecurityPolicy everywhere
    podSecurityPolicy:
      enabled: false
    kube-state-metrics:
      podSecurityPolicy:
        enabled: false
    alertmanager:
      podSecurityPolicy:
        enabled: false

documentation:
  enabled: false

filter_in:
  enabled: true
  path: /usr/lib/cgi-bin
  service: securityIn

filter_out:
  enabled: true
  path: /usr/lib/cgi-bin
  service: securityOut

# Value overrides for the file assets included within the chart template.
files:
  # Directory `files/cwlwrapper-assets` - assets for ConfigMap `XXX-cwlwrapper-config`
  cwlwrapperAssets: {}
    # main.yaml: ""
    # rules.yaml: ""
    # stagein.yaml: ""
    # stageout.yaml: ""

# Provide custom contribution to the zoo configuration files.
customConfig:
  # file main.cfg
  main: {}
#
# Example - passing through custom config that can be accessed directly in a
# cookiecutter template that is provided as a deployment customisation.
# The cookiecutter must be coded to expect these passed-thru parameters by name,
# i.e. under the keys ["myCustomConfig"]["domain"], etc.
#
# ---
# customConfig:
#   main:
#     myCustomConfig: |-
#       domain=mycluster.myplatform
#       workspace_prefix=ws
#     moreCustomConfig: |-
#       something=some-data
#       otherthing=stuff