# ZOO-Project-DRU Helm Chart Configuration with Argo Workflows support


# # Optimization: Use shorter names to reduce Helm secret size
# nameOverride: "zoo"
# fullnameOverride: "zoo"

# Optimization: Disable optional components to reduce Helm secret size (avoid >1MB limit)
# These optimizations reduce secret size by disabling large ConfigMaps
zoo:
  openapi:
    enabled: false  # Disable OpenAPI spec (saves ~29KB)
  security:
    enabled: false  # Disable security service (saves ~11KB)  
  includeOptionalConfigs: false  # Disable web assets (saves ~2KB)

argo:
  enabled: true
  installCRDs: true  # Install CRDs automatically
  instanceID: "zoo"

  # Images configuration
  cwlwrapperImage: "eoepca/cwl-wrapper:0.12.1"
  stageOutImage: "ghcr.io/eoap/mastering-app-package/stage:1.1.0"
  
  # Service Account
  serviceAccount:
    name: "argo-workflow"
    
  # Feature collection script
  featureCollectionScript: ""
  
  # Argo Events configuration
  events:
    enabled: false
    eventBus:
      enabled: false
      name: "default"
      type: "jetstream"  # or "nats"
      jetstream:
        version: "latest"
        replicas: 1  # Reduced for minikube
        settings: |
          max_memory_store: 256MB
          max_file_store: 1GB
        streamConfig: |
          maxMsgs: 10000
          maxAge: 24h
          maxBytes: -1
          replicas: 1
          duplicates: 300s

  autoTokenManagement: true  # Enable automatic token retrieval
  restartOnTokenUpdate: false  # Restart pods after token update
  
  # S3 configuration for artifact repository
  s3:
    bucket: "eoepca"
    endpoint: "s3-service.zoo.svc.cluster.local:9000"
    insecure: true
    # MinIO secret configuration
    secretName: "s3-service"
    accessKeySecretKey: "rootUser"
    secretKeySecretKey: "rootPassword"
    
  # Ingress configuration for Argo Workflows UI
  ingress:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: argo-workflows.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
    #  - secretName: argo-workflows-tls
    #    hosts:
    #      - argo-workflows.local


workflow:
  argo:
    # Deploy Argo workflows using the Helm chart
    internal: true

    # Argo Workflows mode selection
    enabled: true
    instanceID: "zoo"
    
    
    # Common configuration
    defaultVolumeSize: "12Gi"
    defaultMaxRam: "2Gi"
    wfServer: "http://zoo-project-dru-argo-workflows-server.zoo.svc.cluster.local:2746"
    wfToken: "" # Token will be retrieved automatically during deployment
    wfNamespace: "zoo"
    wfSynchronizationCm: "semaphore-argo-cwl-runner-stage-in-out"
    CwlRunnerTemplare: "argo-cwl-runner-stage-in-out"
    CwlRunnerEndpoint: "calrissian-runner"
    # Options for automatic token management
      
  additionalInputs:
    s3_bucket: results
    region_name: us-east-1
    # Use template helpers to get credentials dynamically
    aws_secret_access_key: "minio-secret-password"
    aws_access_key_id: "minio-admin"
    endpoint_url: http://s3-service.zoo.svc.cluster.local:9000

minio:
  # minio chart value parameters description can be found here:
  # https://github.com/bitnami/charts/tree/main/bitnami/minio
  enabled: true
  # Authentication
  rootUser: minio-admin
  rootPassword: minio-secret-password
  persistence:
    enabled: true
    size: 2Gi
    accessMode: ReadWriteOnce
  defaultBuckets: "eoepca"
  buckets:
    - name: eoepca
      policy: none
      purge: false
    - name: results
      policy: none
      purge: false
  fullnameOverride: "s3-service"

cookiecutter:
  templateUrl: https://github.com/gfenoy/zoo-argo-wf-proc-service-template.git
  templateBranch: feature/use-argo-wf-namespace

# CWL Wrapper configuration for official Argo Workflows
wrapper:
  # Rules configuration (will be templated from files/argo-workflows/rules.yaml if not provided)
  rules: ""
  # Main CWL configuration (will be templated from files/argo-workflows/main.yaml if not provided)
  main: ""
  # Stage-in CWL configuration (will be templated from files/argo-workflows/stage-in.yaml if not provided)
  stageIn: ""
  # Stage-out CWL configuration (will be templated from files/argo-workflows/stage-out.yaml if not provided)
  stageOut: ""

iam:
  enabled: false
  openeoAuth:
    enabled: false

webui:
  enabled: false

websocketd:
  enabled: true

redis:
  enabled: true

postgresql:
  enabled: true
  auth:
    database: zoo
    username: zoo
    password: zoo-password
  external:
    enabled: false

# Configuration for official Argo Workflows chart
argo-workflows:
  # Global configuration
  fullnameOverride: "zoo-project-dru-argo-workflows"
  singleNamespace: true
  
  # Image versions
  images:
    tag: "v3.7.1"
    pullPolicy: IfNotPresent
  
  # Global artifact repository configuration
  artifactRepository:
    archiveLogs: true
    s3:
      endpoint: s3-service.zoo.svc.cluster.local:9000
      bucket: eoepca
      insecure: true
      accessKeySecret:
        name: s3-service
        key: rootUser
      secretKeySecret:
        name: s3-service
        key: rootPassword
  
  # Workflow Controller Configuration
  controller:
    enabled: true
    instanceID:
      enabled: true
      useReleaseName: false
      explicitID: "zoo"
    
    # Metrics configuration
    metricsConfig:
      enabled: true
      path: /metrics
      port: 9090
      bindAddress: "0.0.0.0"
    
    # ServiceMonitor configuration for controller metrics
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: zoo-project-dru
    
    # Enable detection of artifact-repositories ConfigMaps
    extraArgs:
      - --managed-namespace=zoo
      - --namespaced
    
    # Disable ClusterWorkflowTemplates in namespaced mode
    clusterWorkflowTemplates:
      enabled: false
    
    # Basic configuration to avoid RBAC issues
    workflowDefaults:
      spec:
        serviceAccountName: argo-workflow
        # TTL configuration for automatic workflow and pod cleanup
        ttlStrategy:
          secondsAfterCompletion: 3600  # Delete workflows after 1h of completion
          secondsAfterSuccess: 300      # Delete successful workflows after 5 minutes  
          secondsAfterFailure: 3600     # Keep failed workflows for 1h for debugging
        # Configuration for automatic pod cleanup
        podGC:
          strategy: OnPodCompletion     # Delete pods as soon as they complete
          deleteDelayDuration: 60s     # Wait 60s before deletion (to retrieve logs)
    
    # Worker configuration for TTL and pod cleanup management
    workflowTTLWorkers: 4    # Number of workers to handle TTL
    podCleanupWorkers: 4     # Number of workers to clean up pods
    
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
  
  # Argo Server Configuration
  server:
    enabled: true
    serviceType: ClusterIP
    servicePort: 2746
    authModes:
      - server
    secure: false
    
    # Namespace configuration - IMPORTANT to avoid RBAC issues
    namespaced: true
    
    # Disable ClusterWorkflowTemplates for the server too
    clusterWorkflowTemplates:
      enabled: false
    
    # Additional configuration to force namespaced mode and disable cluster-wide features
    extraArgs:
      - --namespaced
      - --managed-namespace=zoo
    
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
  
  # ServiceMonitor configuration for metrics collection by Prometheus
  serviceMonitor:
    enabled: true
    additionalLabels:
      release: zoo-project-dru
  
  # RBAC Configuration
  rbac:
    create: true
  
  # Service Account Configuration
  serviceAccount:
    create: true
  
  # Disable CRD installation as they already exist
  crds:
    install: false
    keep: true

# Note: Monitoring stack disabled to keep under 1MB Helm secret limit
# To enable monitoring, deploy kube-prometheus-stack separately:
# helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring --create-namespace
monitoring:
  enabled: false
  # Disable patch since node-exporter is already correctly configured below
  nodeExporterPatchEnabled: true
  kube-prometheus-stack:
    # Configuration for Docker Desktop
    prometheusOperator:
      enabled: true
      podSecurityPolicy:
        enabled: false
    crds:
      enabled: true
    prometheus-node-exporter:
      enabled: true  # Enabled with correct port configuration
      hostNetwork: false
      hostPID: false
      service:
        port: 9101
        targetPort: 9101
      prometheus:
        monitor:
          enabled: true
      # Override the default arguments completely
      extraArgs:
        - --web.listen-address=0.0.0.0:9101
      # Configure container port
      containerPorts:
        http-metrics:
          containerPort: 9101
    
    # Prometheus configuration
    prometheus:
      enabled: true
      prometheusSpec:
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        ruleSelector: {}
    
    # Grafana (heavily optimized to reduce secret size)
    grafana:
      enabled: true
      defaultDashboardsEnabled: false  # Disable all default dashboards
      adminPassword: admin
      sidecar:
        dashboards:
          enabled: false  # Disable dashboard sidecar completely
    
    # Disable all default dashboard ConfigMaps that cause size issues
    defaultRules:
      create: false  # Disable default PrometheusRules (saves ~200KB)
    
    # Disable other dashboard-related components
    kubeApiServer:
      enabled: false
    kubelet:
      enabled: false
    kubeControllerManager:
      enabled: false  
    coreDns:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: false
    kubeStateMetrics:
      enabled: false
    nodeExporter:
      enabled: false
    
    # Kube-state-metrics (disabled to reduce secret size)
    kube-state-metrics:
      enabled: false  # Saves ~100KB in PrometheusRules
    
    # Alertmanager (disabled to reduce secret size) 
    alertmanager:
      enabled: false  # Saves ~50KB in configuration
    
    # Disable PodSecurityPolicy globalement
    podSecurityPolicy:
      enabled: false

# Argo Events disabled to reduce Helm secret size
# Deploy separately if needed: helm install argo-events argo/argo-events
argo-events:
  enabled: false
  
  # Désactiver l'installation des CRDs (ils sont déjà installés)
  crds:
    install: false
    keep: true
  
  # Controller configuration
  controller:
    replicas: 1
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 256Mi
    
    # Metrics
    metrics:
      enabled: true
      port: 8080
      
    # Service Monitor for Prometheus
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: zoo-project-dru
  
  # Event Bus configuration (will use our custom one)
  eventBusConfig:
    jetstream:
      versions:
        - version: latest
          
  # Global configuration
  global:
    image:
      tag: "v1.9.1"
