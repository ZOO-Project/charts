cwlVersion: v1.0

class: CommandLineTool
id: stage-out

doc: "Stage-out the results to S3"
#hints:
#  "cwltool:Secrets":
#    secrets:
#      - STAGEOUT_AWS_ACCESS_KEY_ID
#      - STAGEOUT_AWS_SECRET_ACCESS_KEY
#      - STAGEOUT_AWS_REGION
#      - STAGEOUT_AWS_SERVICEURL
inputs:
  STAGEOUT_OUTPUT:
    type: string
  process:
    type: string
  STAGEOUT_AWS_ACCESS_KEY_ID:
    type: string
  STAGEOUT_AWS_SECRET_ACCESS_KEY:
    type: string
  STAGEOUT_AWS_REGION:
    type: string
  STAGEOUT_AWS_SERVICEURL:
    type: string
  stac_catalog:
    doc: "The folder containing the STAC catalog to stage out"
    label: "STAC Catalog folder"
    type: Directory
outputs:
  s3_catalog_output:
    type: https://raw.githubusercontent.com/eoap/schemas/main/string_format.yaml#URI
    outputBinding:
      glob: catalog-uri.txt
      loadContents: true
      outputEval: |
        ${ 
          return { "value": self[0].contents, "type": "https://raw.githubusercontent.com/eoap/schemas/main/string_format.yaml#URI" };
        }
stdout: catalog-uri.txt
baseCommand:
  - python
  - stage.py
arguments:
  - $( inputs.stac_catalog.path )
  - $( inputs.STAGEOUT_OUTPUT )
  - ${
      var firstPart = (Math.random() * 46656) | 0;
      var secondPart = (Math.random() * 46656) | 0;
      firstPart = ("000" + firstPart.toString(36)).slice(-3);
      secondPart = ("000" + secondPart.toString(36)).slice(-3);
      return inputs.process + "-" + firstPart + secondPart;
    } 

requirements:
  NetworkAccess:
    networkAccess: true
  SchemaDefRequirement:
    types:
    - $import: https://raw.githubusercontent.com/eoap/schemas/main/string_format.yaml
  DockerRequirement:
    dockerPull: ghcr.io/eoap/mastering-app-package/stage:1.0.0
  InlineJavascriptRequirement: {}
  EnvVarRequirement:
    envDef:
      STAGEOUT_AWS_ACCESS_KEY_ID: $( inputs.STAGEOUT_AWS_ACCESS_KEY_ID )
      STAGEOUT_AWS_SECRET_ACCESS_KEY: $( inputs.STAGEOUT_AWS_SECRET_ACCESS_KEY )
      STAGEOUT_AWS_REGION: $( inputs.STAGEOUT_AWS_REGION )
      STAGEOUT_AWS_SERVICEURL: $( inputs.STAGEOUT_AWS_SERVICEURL )
  ResourceRequirement: {}
  InitialWorkDirRequirement:
    listing:
      - entryname: stage.py
        entry: |-
          import os
          import sys
          import pystac
          import botocore
          import boto3
          import shutil
          from pystac.stac_io import DefaultStacIO, StacIO
          from urllib.parse import urlparse
          from datetime import datetime

          cat_url = sys.argv[1]
          bucket = sys.argv[2]
          subfolder = sys.argv[3]
          collection_id = subfolder

          print(f"cat_url: {cat_url}", file=sys.stderr)
          print(f"bucket: {bucket}", file=sys.stderr)
          print(f"subfolder: {subfolder}", file=sys.stderr)

          aws_access_key_id = os.environ["STAGEOUT_AWS_ACCESS_KEY_ID"]
          aws_secret_access_key = os.environ["STAGEOUT_AWS_SECRET_ACCESS_KEY"]
          region_name = os.environ["STAGEOUT_AWS_REGION"]
          endpoint_url = os.environ["STAGEOUT_AWS_SERVICEURL"]

          shutil.copytree(cat_url, "/tmp/catalog")
          cat = pystac.read_file(os.path.join("/tmp/catalog", "catalog.json"))

          class CustomStacIO(DefaultStacIO):
              """Custom STAC IO class that uses boto3 to read from S3."""

              def __init__(self):
                  self.session = botocore.session.Session()
                  self.s3_client = self.session.create_client(
                      service_name="s3",
                      use_ssl=True,
                      aws_access_key_id=aws_access_key_id,
                      aws_secret_access_key=aws_secret_access_key,
                      endpoint_url=endpoint_url,
                      region_name=region_name,
                  )

              def write_text(self, dest, txt, *args, **kwargs):
                  parsed = urlparse(dest)
                  if parsed.scheme == "s3":
                      self.s3_client.put_object(
                          Body=txt.encode("UTF-8"),
                          Bucket=parsed.netloc,
                          Key=parsed.path[1:],
                          ContentType="application/geo+json",
                      )
                  else:
                      super().write_text(dest, txt, *args, **kwargs)


          client = boto3.client(
              "s3",
              aws_access_key_id=aws_access_key_id,
              aws_secret_access_key=aws_secret_access_key,
              endpoint_url=endpoint_url,
              region_name=region_name,
          )

          StacIO.set_default(CustomStacIO)

          # create a STAC collection for the process
          date = datetime.now().strftime("%Y-%m-%d")

          dates = [datetime.strptime(
              f"{date}T00:00:00", "%Y-%m-%dT%H:%M:%S"
          ), datetime.strptime(f"{date}T23:59:59", "%Y-%m-%dT%H:%M:%S")]

          collection = pystac.Collection(
            id=collection_id,
            description="description",
            extent=pystac.Extent(
              spatial=pystac.SpatialExtent([[-180, -90, 180, 90]]), 
              temporal=pystac.TemporalExtent(intervals=[[min(dates), max(dates)]])
            ),
            title="Processing results",
            href=f"s3://{bucket}/{subfolder}/collection.json",
            stac_extensions=[],
            keywords=["eoepca"],
            license="proprietary",
          )

          for index, link in enumerate(cat.links):
            if link.rel == "root":
                cat.links.pop(index) # remove root link

          for item in cat.get_items():

              item.set_collection(collection)

              collection.add_item(item)

              for key, asset in item.get_assets().items():
                  s3_path = os.path.normpath(
                      os.path.join(subfolder, collection_id, item.id, os.path.basename(asset.href))
                  )
                  print(f"upload {asset.href} to s3://{bucket}/{s3_path}",file=sys.stderr)
                  client.upload_file(
                      asset.get_absolute_href(),
                      bucket,
                      s3_path,
                  )
                  asset.href = f"s3://{bucket}/{s3_path}"
                  item.add_asset(key, asset)

          collection.update_extent_from_items() 

          cat.clear_items()

          cat.add_child(collection)

          cat.normalize_hrefs(f"s3://{bucket}/{subfolder}")

          for item in collection.get_items():
              # upload item to S3
              print(f"upload {item.id} to s3://{bucket}/{subfolder}", file=sys.stderr)
              pystac.write_file(item, item.get_self_href())

          # upload collection to S3
          print(f"upload collection.json to s3://{bucket}/{subfolder}", file=sys.stderr)
          pystac.write_file(collection, collection.get_self_href())

          # upload catalog to S3
          print(f"upload catalog.json to s3://{bucket}/{subfolder}", file=sys.stderr)
          pystac.write_file(cat, cat.get_self_href())

          print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)