cwlVersion: v1.0


class: CommandLineTool
id: stage-out

doc: "Stage-out the results to S3"
inputs:
  s3_bucket:
    type: string
  sub_path:
    type: string
  aws_access_key_id:
    type: string
  aws_secret_access_key:
    type: string
  region_name:
    type: string
  endpoint_url:
    type: string
outputs:
  s3_catalog_output:
    outputBinding:
      outputEval: ${  return "s3://" + inputs.s3_bucket + "/" + inputs.sub_path + "/catalog.json"; }
    type: string
baseCommand:
  - python
  - stage.py
arguments:
  - $( inputs.wf_outputs.path )
  - $( inputs.s3_bucket )
  - $( inputs.sub_path )
requirements:
  DockerRequirement:
    dockerPull: ghcr.io/eoap/mastering-app-package/stage:1.1.0
  InlineJavascriptRequirement: {}
  EnvVarRequirement:
    envDef:
      aws_access_key_id: $( inputs.aws_access_key_id )
      aws_secret_access_key: $( inputs.aws_secret_access_key )
      aws_region_name: $( inputs.region_name )
      aws_endpoint_url: $( inputs.endpoint_url )
  ResourceRequirement: {}
  InitialWorkDirRequirement:
    listing:
      - entryname: stage.py
        entry: |-
          import os
          import sys
          import pystac
          import botocore
          import boto3
          import shutil
          from loguru import logger
          from pystac.stac_io import DefaultStacIO, StacIO
          from urllib.parse import urlparse
          from datetime import datetime
          from pystac.extensions.item_assets import ItemAssetsExtension, AssetDefinition

          cat_url = sys.argv[1]
          bucket = sys.argv[2]
          subfolder = sys.argv[3]

          aws_access_key_id = os.environ["aws_access_key_id"]
          aws_secret_access_key = os.environ["aws_secret_access_key"]
          region_name = os.environ["aws_region_name"]
          endpoint_url = os.environ["aws_endpoint_url"]

          logger.info(f"stage-out {cat_url} to s3://{bucket}/{subfolder}")

          shutil.copytree(cat_url, "/tmp/catalog")
          cat = pystac.read_file(os.path.join("/tmp/catalog", "catalog.json"))

          logger.info(f"catalog {cat}")

          class CustomStacIO(DefaultStacIO):
              """Custom STAC IO class that uses boto3 to read from S3."""

              def __init__(self):
                  self.session = botocore.session.Session()
                  self.s3_client = self.session.create_client(
                      service_name="s3",
                      use_ssl=True,
                      aws_access_key_id=aws_access_key_id,
                      aws_secret_access_key=aws_secret_access_key,
                      endpoint_url=endpoint_url,
                      region_name=region_name,
                  )

              def write_text(self, dest, txt, *args, **kwargs):
                  parsed = urlparse(dest)
                  if parsed.scheme == "s3":
                      self.s3_client.put_object(
                          Body=txt.encode("UTF-8"),
                          Bucket=parsed.netloc,
                          Key=parsed.path[1:],
                          ContentType="application/geo+json",
                      )

                  else:
                      super().write_text(dest, txt, *args, **kwargs)


          client = boto3.client(
              "s3",
              aws_access_key_id=aws_access_key_id,
              aws_secret_access_key=aws_secret_access_key,
              endpoint_url=endpoint_url,
              region_name=region_name,
          )

          StacIO.set_default(CustomStacIO)

          # create a STAC collection for the process
          collection_id = "results-collection"

          date = datetime.now().strftime("%Y-%m-%d")

          dates = [
              datetime.strptime(f"{date}T00:00:00", "%Y-%m-%dT%H:%M:%S"),
              datetime.strptime(f"{date}T23:59:59", "%Y-%m-%dT%H:%M:%S"),
          ]

          collection = pystac.Collection(
              id=collection_id,
              description="description",
              extent=pystac.Extent(
                  spatial=pystac.SpatialExtent([[-180, -90, 180, 90]]),
                  temporal=pystac.TemporalExtent(intervals=[[min(dates), max(dates)]]),
              ),
              title="Processing results",
              href=f"s3://{bucket}/{subfolder}/{collection_id}/collection.json",
              stac_extensions=[],
              keywords=["keyword1", "keyword2"],
              license="proprietary",
          )


          for item in cat.get_items():

              item.set_collection(collection)
              collection.add_item(item)
              for key, asset in item.get_assets().items():
                  s3_path = os.path.normpath(
                      os.path.join(os.path.join(subfolder, collection_id, asset.href.replace("/tmp/catalog/","")))
                  )
                  client.upload_file(
                      asset.get_absolute_href(),
                      bucket,
                      s3_path
                  )
                  asset.href = f"s3://{bucket}/{s3_path}"
                  item.add_asset(key, asset)
          collection.update_extent_from_items()


          # Access the item-assets extension
          item_assets_ext = ItemAssetsExtension.ext(collection, add_if_missing=True)
          if ItemAssetsExtension.get_schema_uri() not in collection.stac_extensions:
              collection.stac_extensions.append(ItemAssetsExtension.get_schema_uri())

          item_assets = {}
          for item in collection.get_items():
              # Loop over the assets in the item and create AssetDefinitions for each
              for asset_key, asset in item.assets.items():
                  # Create AssetDefinition from existing asset properties
                  # remove the statistics and histogram from the extra fields (raster extension)

                  # Switch the with_raster parameter to activate the raster extension from there:
                  # https://github.com/Terradue/ogc-eo-application-package-hands-on/blob/master/water-bodies/command-line-tools/stac/app.py#L46
                  # The following 2 lines should be commented if the raster extension is not used
                  #asset.extra_fields["raster:bands"][0].pop("statistics")
                  #asset.extra_fields["raster:bands"][0].pop("histogram")
                  

                  asset_definition = AssetDefinition.create(
                      title=asset.title,
                      description=asset.description,
                      media_type=asset.media_type,
                      roles=asset.roles,
                      extra_fields=asset.extra_fields,
                  )
                  # Add the asset definition to the collection's item assets
                  item_assets[asset_key] = asset_definition

          cat.clear_items()

          cat.add_child(collection)

          cat.normalize_hrefs(f"s3://{bucket}/{subfolder}")

          for item in collection.get_items():
              for index, link in enumerate(item.links):
                  if link.rel in ["root"]:
                      item.links.pop(index)
              # upload item to S3
              logger.info(f"upload {item.id} to s3://{bucket}/{subfolder}")
              pystac.write_file(item, item.get_self_href())

          # upload collection to S3
          logger.info(f"upload collection.json to s3://{bucket}/{subfolder}")
          for index, link in enumerate(collection.links):
              if link.rel in ["root"]:
                  collection.links.pop(index)
          pystac.write_file(collection, collection.get_self_href())

          # upload catalog to S3
          logger.info(f"upload catalog.json to s3://{bucket}/{subfolder}")
          for index, link in enumerate(cat.links):
              if link.rel in ["root"]:
                  cat.links.pop(index)
          pystac.write_file(cat, cat.get_self_href())

          shutil.rmtree("/tmp/catalog/")

          print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)
