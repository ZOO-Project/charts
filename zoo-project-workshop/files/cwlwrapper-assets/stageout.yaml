cwlVersion: v1.0
class: CommandLineTool
id: stage-out
doc: "Stage-out the results to S3"
#hints:
#  "cwltool:Secrets":
#    secrets:
#      - STAGEOUT_AWS_ACCESS_KEY_ID
#      - STAGEOUT_AWS_SECRET_ACCESS_KEY
#      - STAGEOUT_AWS_REGION
#      - STAGEOUT_AWS_SERVICEURL
inputs:
  STAGEOUT_OUTPUT:
    type: string
  process:
    type: string
  STAGEOUT_AWS_ACCESS_KEY_ID:
    type: string
  STAGEOUT_AWS_SECRET_ACCESS_KEY:
    type: string
  STAGEOUT_AWS_REGION:
    type: string
  STAGEOUT_AWS_SERVICEURL:
    type: string
outputs:
  StacCatalogUri:
    outputBinding:
      outputEval: ${  return inputs.STAGEOUT_OUTPUT + "/" + inputs.process + "/catalog.json"; }
    type: string
baseCommand:
  - python
  - stage.py
arguments:
  - $( inputs.wf_outputs.path )
  - $( inputs.STAGEOUT_OUTPUT )
  - $( inputs.process )
requirements:
  DockerRequirement:
    dockerPull: ghcr.io/terradue/ogc-eo-application-package-hands-on/stage:1.3.2
  InlineJavascriptRequirement: {}
  EnvVarRequirement:
    envDef:
      STAGEOUT_AWS_ACCESS_KEY_ID: $( inputs.STAGEOUT_AWS_ACCESS_KEY_ID )
      STAGEOUT_AWS_SECRET_ACCESS_KEY: $( inputs.STAGEOUT_AWS_SECRET_ACCESS_KEY )
      STAGEOUT_AWS_REGION: $( inputs.STAGEOUT_AWS_REGION )
      STAGEOUT_AWS_SERVICEURL: $( inputs.STAGEOUT_AWS_SERVICEURL )

  ResourceRequirement: {}
  InitialWorkDirRequirement:
    listing:
      - entryname: stage.py
        entry: |-
          import os
          import sys
          import pystac
          import botocore
          import boto3
          import shutil
          from pystac.stac_io import DefaultStacIO, StacIO
          from urllib.parse import urlparse

          cat_url = sys.argv[1]
          bucket = sys.argv[2].replace("s3://","")
          subfolder = sys.argv[3]
          
          aws_access_key_id = os.environ["STAGEOUT_AWS_ACCESS_KEY_ID"]
          aws_secret_access_key = os.environ["STAGEOUT_AWS_SECRET_ACCESS_KEY"]
          region_name = os.environ["STAGEOUT_AWS_REGION"]
          endpoint_url = os.environ["STAGEOUT_AWS_SERVICEURL"]

          shutil.copytree(cat_url, "/tmp/catalog")
          cat = pystac.read_file(os.path.join("/tmp/catalog", "catalog.json"))

          class CustomStacIO(DefaultStacIO):
              """Custom STAC IO class that uses boto3 to read from S3."""

              def __init__(self):
                  self.session = botocore.session.Session()
                  self.s3_client = self.session.create_client(
                      service_name="s3",
                      use_ssl=True,
                      aws_access_key_id=aws_access_key_id,
                      aws_secret_access_key=aws_secret_access_key,
                      endpoint_url=endpoint_url,
                      region_name=region_name,
                  )

              def write_text(self, dest, txt, *args, **kwargs):
                  parsed = urlparse(dest)
                  if parsed.scheme == "s3":
                      self.s3_client.put_object(
                          Body=txt.encode("UTF-8"),
                          Bucket=parsed.netloc,
                          Key=parsed.path[1:],
                          ContentType="application/geo+json",
                      )
                  else:
                      super().write_text(dest, txt, *args, **kwargs)


          client = boto3.client(
              "s3",
              aws_access_key_id=aws_access_key_id,
              aws_secret_access_key=aws_secret_access_key,
              endpoint_url=endpoint_url,
              region_name=region_name,
          )

          StacIO.set_default(CustomStacIO)

          for item in cat.get_items():
              for key, asset in item.get_assets().items():
                  s3_path = os.path.normpath(
                      os.path.join(os.path.join(subfolder, item.id, asset.href))
                  )
                  print(f"upload {asset.href} to s3://{bucket}/{s3_path}",file=sys.stderr)
                  client.upload_file(
                      asset.get_absolute_href(),
                      bucket,
                      s3_path,
                  )
              # upload item to S3
              print(f"upload {item.id} to s3://{bucket}/{subfolder}", file=sys.stderr)
              pystac.write_file(item, item.get_self_href())

          cat.normalize_hrefs(f"s3://{bucket}/{subfolder}")

          # upload catalog to S3
          print(f"upload catalog.json to s3://{bucket}/{subfolder}", file=sys.stderr)
          pystac.write_file(cat, cat.get_self_href())

          print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)
